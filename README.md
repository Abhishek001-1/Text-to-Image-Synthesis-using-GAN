#Deep Learning

# Text-to-image-synthesis-using-GAN


# Dataset and feature 
In order to develop and assess models that generate both text and images, it is necessary to have a set of image-text pairs. The COCO Captions Dataset [13] is a commonly used resource for such data, as it contains more than 330,000 images and 1.5 million corresponding captions. The dataset is typically divided into three subsets: around 120K images for training, 5K for validation, and 205K for testing. However, for future research, it might be advantageous to consider the Conceptual Captions dataset [12] created by Google AI, which includes over 3 million images with associated captions in a TSV file. The images in this dataset were resized to 32 x 32 pixels due to computing constraints. A few sample images from the dataset are presented below.

![imggg](https://github.com/Abhishek001-1/Text-to-Image-Synthesis-using-GAN/assets/75797884/ea01d50c-5b31-474b-8f84-899bdb277811)

![255313015-c2ed6eb5-0866-4933-99f2-cb00fd28ad1e](https://github.com/Abhishek001-1/Text-to-Image-Synthesis-using-GAN/assets/75797884/62ead57f-64ca-4345-ab3f-9c583c727597)
